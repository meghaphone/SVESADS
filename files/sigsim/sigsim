#!/Library/Frameworks/Python.framework/Versions/3.8/bin/python3
# Replace path above with path to your python3 executable

# install prerequisites with "pip3 install numpy cv2 sklearn json uuid"

import numpy as np
import cv2
from sklearn.cluster import KMeans
import json
import os
import uuid
import random

import argparse

# Below lists all the help for the subcommands on one page instead of needing 'command subcommand help'.
# See https://stackoverflow.com/questions/20094215/argparse-subparser-monolithic-help-output
class _HelpAction(argparse._HelpAction):

    def __call__(self, ap, namespace, values, option_string=None):
        ap.print_help()
        print()

        # retrieve subparsers from parser
        subparsers_actions = [
            action for action in ap._actions
            if isinstance(action, argparse._SubParsersAction)]
        # there will probably only be one subparser_action,
        # but better save than sorry
        for subparsers_action in subparsers_actions:
            # get all subparsers and print help
            for choice, subparser in subparsers_action.choices.items():
                print("\nsubcommand '{}'".format(choice))
                print(subparser.format_help())

        ap.exit()

# Using K-Means Clustering to find clusters of colors, finding a given number of the most dominant colors
# See https://code.likeagirl.io/finding-dominant-colour-on-an-image-b4e075f98097
def dominant_colors(img, num):
	try:
		img = np.delete(img, 3, 2)								# kill alpha channel
	except:
		pass
	img = img.reshape((img.shape[0] * img.shape[1], 3)) 		#represent as row*column,channel number
	sample = []
	for i in range(10000):
		sample.append(img[ np.random.randint(0,len(img)) ])
	
	clt = KMeans(n_clusters=num)
	clt.fit(sample)
	c = sorted(clt.cluster_centers_, key=sum, reverse=True)		# sort resulting colors from light to dark
	for i in range(len(c)):
		c[i] = list(c[i].astype(int))
	return c



# Mouse events handler for --manual mode
def mouse_events(event, x, y, flags, param):
	global points, redraw, marksize
	if event == cv2.EVENT_LBUTTONDOWN:
		# Figure out if this is a delete by seeing if close to an existing point
		for p in points:
			px, py = p
			dx = abs(x - px)
			dy = abs(y - py)
			if dx < 15 and dy < 15:
				# delete point, let key scan loop handle redraw
				points.remove(p)
				redraw = True
		# if no point deleted then add a new point and draw circle on photo
		if not redraw:
			points.append((x, y))
			cv2.circle(photo, (x, y), marksize, (0,0,255), 3)
			cv2.imshow('marking', photo)


# Below is rotate_bound from imutils's convenience.py, but modified for transparent background
def rotate_bound_on_transparent(image, angle):
	# grab the dimensions of the image and then determine the
	# center
	(h, w) = image.shape[:2]
	(cX, cY) = (w // 2, h // 2)

	# grab the rotation matrix (applying the negative of the
	# angle to rotate clockwise), then grab the sine and cosine
	# (i.e., the rotation components of the matrix)
	M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)
	cos = np.abs(M[0, 0])
	sin = np.abs(M[0, 1])

	# compute the new bounding dimensions of the image
	nW = int((h * sin) + (w * cos))
	nH = int((h * cos) + (w * sin))

	# adjust the rotation matrix to take into account translation
	M[0, 2] += (nW / 2) - cX
	M[1, 2] += (nH / 2) - cY

	# perform the actual rotation, on a transparent background
	return cv2.warpAffine(
		image, M, (nW, nH),
		borderMode=cv2.BORDER_CONSTANT,
		borderValue=(255,255,255,0)
	)
	

def make_signal(shapefile, scale, perspective, rotation):

	global meta

	image = cv2.imread(shapefile, cv2.IMREAD_UNCHANGED)
	
	# perspective

	psp_rot, psp = perspective
	image = rotate_bound_on_transparent(image, psp_rot)
	h, w = image.shape[:2]
	src = np.float32( [ [0, 0], [w-1, 0], [w-1, h-1], [0, h-1] ] )
	dst = np.float32( [ [psp, psp], [w-psp, psp], [w-1, h-1], [0, h-1] ] )
	M = cv2.getPerspectiveTransform(src, dst)
	image = cv2.warpPerspective(image, M, (w, h))
	image = rotate_bound_on_transparent(image, -psp_rot)

	
	# Rotate
	image = rotate_bound_on_transparent(image, rotation)

	# Shrink
	# first add random borders to "jiggle" the image a bit
	h, w = image.shape[:2]
	top = np.random.randint(0, scale)
	bottom = scale - ( (h + top) % scale )
	left = np.random.randint(0, scale)
	right = scale - ( (w + left) % scale )
	image = cv2.copyMakeBorder(image, top, bottom, left, right, borderType=cv2.BORDER_CONSTANT, value=(255,255,255,0))	
	nh = int(h / scale)
	nw = int(w / scale)
	image = cv2.resize(image, (nw, nh), interpolation=cv2.INTER_AREA)


	# Crop the final output image
	# mask is the alpha channel
	mask = image[:,:,3]
	# clip any rows or cols with only transparent pixels
	image = image[np.ix_(mask.any(1),mask.any(0))]

	# Add transparent pixels for mask
	image = cv2.copyMakeBorder(image, 3, 3, 3, 3, borderType=cv2.BORDER_CONSTANT, value=(255,255,255,0))

	# Redistribute transparency
	rows, cols = image.shape[:2]
	for i in range(rows):
		for j in range(cols):
			# transparency values 0-255 redistributed to 128-255
			if image[i][j][3] > 0:
				image[i][j][3] = 128 + ( image[i][j][3] / 256 ) * 128
				
	return image
	

def make_mask(image):
	mask = image[:,:,3]
	mask = cv2.blur(mask,(5,5))
	h, w = image.shape[:2]
	for i in range(h):
		for j in range(w):
			if mask[i][j]: mask[i][j] = 255
	return mask

	
def merge_signal(photo, signal, p, lighter, to_color):
	x, y = p
	x = int(x)
	y = int(y)
	h, w = signal.shape[:2]
	sx = int(x - (w / 2))
	sy = int(y - (h / 2))
	for i in range(h):
		for j in range(w):
			photopixel = photo[sy + i][sx + j]
			alpha = signal[i][j][3] / 255
			for c in range(3):
				pix = photo[sy + i][sx + j][c]
				sig = to_color[c]
				photo[sy + i][sx + j][c] = to_byte( pix + ( (lighter / 100) * alpha * abs(sig - pix) ) )
	
	cv2.circle(photo, (x, y), 20, (0,0,255), 3)
	return photo

	
def normal_dist(mean, sd, minimum=0, maximum=255):
	r = int(np.random.normal(mean, sd))
	return max(min(r, maximum), minimum)
	
def to_byte(value):
	return max(min(int(value), 255), 0)

def place_mark(x = -1, y = -1, forced = False):

	global photo, training_data, resolution, domcol

	minsize, maxsize = range_split(args.sizerange)
	scale = int( resolution / np.random.uniform(float(minsize), float(maxsize)) )
	
	if not args.lighter and not args.darker:
		print("Must specify one or both of --lighter or --darker to indicate what kind of marks to place")
		quit()		
	cointoss = np.random.randint(0,2)
	if args.lighter:
		minlighter, maxlighter = range_split(args.lighter)	
		lighter = np.random.randint(int(minlighter), int(maxlighter) + 1)
		to_color = domcol[np.random.randint(0,3)]
	if args.darker and (not args.lighter or cointoss):
		mindarker, maxdarker = range_split(args.darker)	
		lighter = - np.random.randint(int(mindarker), int(maxdarker) + 1)
		to_color = domcol[len(domcol) - np.random.randint(1,4)]
		
	perspective = ( np.random.randint(0,360), np.random.randint(0, args.perspective) )
	rotation = np.random.randint(0,360)

	signal = make_signal(args.shape, scale, perspective, rotation)
	mask = make_mask(signal)
	sh, sw = signal.shape[:2]
	minx = sw / 2
	miny = sh / 2
	maxx = pw - (sw / 2)
	maxy = ph - (sh / 2)

	count = 0
	if x == -1: 
		randomspot = True
	else:
		randomspot = False
	while True:
		if randomspot:
			x = np.random.randint(minx, maxx)
			y = np.random.randint(miny, maxy)

		sx = int(x - (sw / 2))
		sy = int(y - (sh / 2))
		#crop photo to size of mask
		roi = photo[sy:sy+sh, sx:sx+sw]
		mean, stddev = cv2.meanStdDev(roi, mask=mask)
		mean = mean.reshape(1, -1)[0]
		mean = mean[0:3]
		area_lightness = np.mean(mean)
		target_lightness = np.mean(to_color)
				
		if ( max(stddev) > args.clearlanding and not forced ) or \
		( sum(abs(mean - to_color)) < args.contrast ):
			count += 1
			if count > 10000:
				print ("Cannot find landing spot")
				break
		else:
			print (mean)
			photo = merge_signal(photo, signal, (x, y), lighter, to_color)
			print ("Placed mark at %d,%d: lighter %d, to_color %s" % (x, y, lighter, to_color))
			if args.output:
				training_data.write('  <object>\n    <name>signal</name>\n    <difficult>0</difficult>\n    <bndbox>\n')
				training_data.write('      <xmin>' + str(sx) + '</xmin>\n')
				training_data.write('      <ymin>' + str(sy) + '</ymin>\n')
				training_data.write('      <xmax>' + str(sx + sw) + '</xmax>\n')
				training_data.write('      <ymax>' + str(sy + sh) + '</ymax>\n')
				training_data.write('    </bndbox>\n  </object>\n')				
			break;



def range_split(s):
	if s.find('-') != -1:
		return s.split('-')
	else:
		return s, s


# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser(add_help=False)
ap.add_argument('--help', '-h',
	action=_HelpAction,
	help='Shows this help text')
sp = ap.add_subparsers(dest='subcommand',
	help='You must specify one of these subcommands')

manual = sp.add_parser('manual', add_help=False)
manual.add_argument('--input', '-i',
	help='input aerial image file',
	required=True,
	metavar='<inputfile>')
manual.add_argument('--resolution',
	metavar='<pixelsize>',
	help='Specify image resolution in centimeters per pixel, must specify if no image metadata exists yet',
	type=int)

debug = sp.add_parser('debug', add_help=False)
debug.add_argument('--resolution',
	metavar='<pixelsize>',
	help='Specify image resolution in centimeters per pixel',
	required=True,
	type=int)
debug.add_argument('--shape', '-s',
	help='path to shape file for signal marks. 1 px = 1 cm, white on transparent bg',
	required=True,
	metavar='<shapefile>')
debug.add_argument('--output', '-o',
	help='output file, defaults to show on screen. If the word RANDOM (in all caps) appears anywhere, it will be replaced by eight random hex characters so you can write to unique random files.',
	metavar='<outputfile>')
debug.add_argument('--rotate', '-r',
	help='rotation in degrees, defaults to random rotation',
	metavar='<deg>',
	type=int,
	default=-1)
debug.add_argument('--perspective', '-p',
	help='Maximum random perspective shift in cm',
	metavar='<cm>',
	type=int,
	default=100)

mark = sp.add_parser('mark', add_help=False)
mark.add_argument('--shape', '-s',
	help='path to shape file for signal marks. 1 px = 1 cm, white on transparent bg',
	required=True,
	metavar='<shapefile>')
mark.add_argument('--input', '-i',
	help='input aerial image file',
	required=True,
	metavar='<inputfile>')
mark.add_argument('--resolution',
	metavar='<pixelsize>',
	help='Specify image resolution in centimeters per pixel',
	type=int)
mark.add_argument('--output', '-o',
	help='output file, defaults to show on screen. If the word RANDOM (in all caps) appears anywhere, it will be replaced by eight random hex characters so you can write to unique random files.',
	metavar='<outputfile>')
mark.add_argument('--random',
	help='add between <from> and <to> randomly located signals',
	metavar='<from>-<to>',
	default='0-0',
	type=str)
mark.add_argument('--clearlanding', '-l',
	help='Maximum stddev in any color channel for area to be suitably quiet for signal placement',
	metavar='<stddev>',
	default=10,
	type=int)
mark.add_argument('--contrast', '-c',
	help="Contrast as specified as the minimum sum total of the differences in the three color channels between mark and background color, default: 150",
	metavar='<contrast>',
	default=150,
	type=int)
mark.add_argument('--lighter',
	help='Create marks by randomly picking a value in this range and then adding it to the RGB values for pixels in the mark area.',
	metavar='<from>-<to>',
	type=str)
mark.add_argument('--darker',
	help='Create marks by randomly picking a value in this range and then subtracting it from the RGB values for pixels in the mark area.',
	metavar='<from>-<to>',
	type=str)
mark.add_argument('--perspective', '-p',
	help='Maximum random perspective shift in cm',
	metavar='<cm>',
	type=int,
	default=100)
mark.add_argument('--sizerange',
	help='Random variation in signal size, e.g.: --sizerange 0.8-1.5',
	metavar='<smallest-biggest>',
	type=str,
	default='0.9-1.1')
	
crop = sp.add_parser('crop', add_help=False)
crop.add_argument('--input', '-i',
	help='input aerial image file',
	required=True,
	metavar='<inputfile>')
crop.add_argument('--output', '-o',
	help='output file, defaults to show on screen. If the word RANDOM (in all caps) appears anywhere, it will be replaced by eight random hex characters so you can write to unique random files.',
	metavar='<outputfile>')	
crop.add_argument('--width', '-w',
	help='width of cropped area',
	metavar='<width>',
	default=6545,
	type=int)
crop.add_argument('--height', '-h',
	help='height of cropped area',
	metavar='<height>',
	default=3417,
	type=int)
crop.add_argument('-x',
	help='x of left top',
	metavar='<x>',
	default=0,
	type=int)
crop.add_argument('-y',
	help='y of left top',
	metavar='<y>',
	default=54,
	type=int)
	
chop = sp.add_parser('chop', add_help=False)
chop.add_argument('--input', '-i',
	help='input aerial image file',
	required=True,
	metavar='<inputfile>')
chop.add_argument('--output', '-o',
	help='output file, defaults to show on screen. If the word RANDOM (in all caps) appears anywhere, it will be replaced by eight random hex characters so you can write to unique random files.',
	metavar='<outputfile>')	
chop.add_argument('--width', '-w',
	help='width of each chunk',
	metavar='<width>',
	default=300,
	type=int)
chop.add_argument('--height', '-h',
	help='height of each chunk',
	metavar='<height>',
	default=300,
	type=int)

args = ap.parse_args()


# manual subcommand
# (Works in conjunction with mouse handler mouse_events above)

if args.subcommand == 'manual':

	cv2.namedWindow('marking', cv2.WINDOW_NORMAL)
	cv2.setMouseCallback('marking', mouse_events)
	
	# See if we have a metadate file <imagefilename.json>
	if os.access(args.input + '.json', os.R_OK):
		with open(args.input + '.json', 'r') as f:
			meta = json.load(f)
			points = list(meta['points'])
	else:
		# If no metadata file, initialise using specified resolution and empty points list
		if not args.resolution:
			print('No metadata found, must specify pixel size in centimeters using --resolution.')
			quit()
		meta = dict()
		points = list()
		meta['resolution'] = args.resolution
		
	marksize = int(1000 / meta['resolution'])
			
	# Always (re)draw points after loading image
	# (redraw variable is made True in mouse_events, where it is a global)
	redraw = True
	while True:
		if redraw:
			photo = cv2.imread(args.input, cv2.IMREAD_UNCHANGED)
			for p in points:
				x, y = p
				cv2.circle(photo, (x, y), marksize, (0,0,255), 3)
			cv2.imshow('marking', photo)
			redraw = False
		key = cv2.waitKey(1) & 0xFF		
		if key == ord('q'):
			quit()
		if key == ord('s'):
			print('Saving point data ... ', end = '')
			with open(args.input + '.json', 'w') as f:
				meta['points'] = points
				json.dump(meta, f)
			print('done.')
			quit()
			



# --debug subcommand
# Show only one mark (in greyscale), or write it to output file

if args.subcommand == 'debug':

	angle = args.rotate
	if angle == -1:
		angle = np.random.randint(0, 359)
		
	perspective = ( np.random.randint(0, 359), np.random.randint(0, args.perspective))

	image = make_signal(args.shape, args.resolution, perspective, angle)
	if not args.output:
		mask = make_mask(image)
		cv2.imshow('mask', mask)	
		image = image[:,:,3]
		cv2.imshow('output', image)
		cv2.waitKey()
	else:
		outfile = args.output.replace("RANDOM", str(uuid.uuid4())[0:8])
		cv2.imwrite(outfile, image)
	quit()
	



# mark subcommand

if args.subcommand == 'mark':

	minrandom, maxrandom = range_split(args.random)
	random = np.random.randint(int(minrandom), int(maxrandom) + 1)
	    
	# See if we have a metadate file <imagefilename.json>
	if not os.access(args.input + '.json', os.R_OK):
		if args.resolution:
			resolution = args.resolution
			points = []
		else:
			print('No resolution specified or metadata found.')
			quit()
	else:
		with open(args.input + '.json', 'r') as f:
			meta = json.load(f)
			resolution = meta['resolution']
			points = list(meta['points'])

	photo = cv2.imread(args.input, cv2.IMREAD_UNCHANGED)
	ph, pw = photo.shape[:2]
	
	domcol = dominant_colors(photo, 10)
	
	if args.output:
		outfile = args.output.replace("RANDOM", str(uuid.uuid4())[0:8])
		training_data = open(outfile + '.xml', 'w')
		training_data.write('<annotation>\n  <size>\n    <width>' + str(pw) + '</width>\n    <height>' + str(ph) + '</height>\n  </size>\n')
	
	for p in points:
		x, y = p
		# Force the mark in this spot with the final argument True
		place_mark(x, y, True)
	
	if random:
		for r in range(random):
			place_mark()

	if not args.output:
		cv2.namedWindow('output', cv2.WINDOW_NORMAL)
		h, w = photo.shape[:2]
		if h > 768 or w > 1024:
			cv2.resizeWindow('output', (1024, 768))
		cv2.imshow('output', photo)
		cv2.waitKey()
	else:
		training_data.write('</annotation>\n')
		training_data.close()
		cv2.imwrite(outfile, photo)
	quit()
	


# crop subcommand

if args.subcommand == 'crop':

	photo = cv2.imread(args.input, cv2.IMREAD_UNCHANGED)
	
	photo = photo[args.y:args.y + args.height, args.x:args.x + args.width]
	
	if not args.output:
		cv2.namedWindow('output', cv2.WINDOW_NORMAL)
		h, w = photo.shape[:2]
		if h > 768 or w > 1024:
			cv2.resizeWindow('output', (1024, 768))
		cv2.imshow('output', photo)
		cv2.waitKey()
	else:
		outfile = args.output.replace("RANDOM", str(uuid.uuid4())[0:8])
		cv2.imwrite(outfile, photo)
	quit()
	
	
# chop subcommand

if args.subcommand == 'chop':

	photo = cv2.imread(args.input, cv2.IMREAD_UNCHANGED)
	h, w = photo.shape[:2]
	
	for i in range(0, w, args.width):
		for j in range(0, h, args.height):
			if w - i > args.width and h - j > args.height:
				crop = photo[j:j + args.height, i:i + args.width]
				if not args.output:
					cv2.imshow('crop', crop)
					cv2.waitKey()
				else:
					outfile = args.output.replace("RANDOM", str(uuid.uuid4())[0:8])
					cv2.imwrite(outfile, crop)
	quit()